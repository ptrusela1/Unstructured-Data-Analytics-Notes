{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "title: \"Homework 1\"\n",
        "author: \"Paul Trusela\"\n",
        "format:\n",
        "  html:\n",
        "    toc: true\n",
        "    toc-location: left\n",
        "    self-contained: true\n",
        "jupyter: python3\n",
        "---\n",
        "\n",
        "\n",
        "Professional wrestling, while not everyone's cup of tea, is big business. What started as a carnival act has turned into a global entertainment industry. Netflix recently started showing Monday Night Raw, a program from the biggest North American wrestling company, WWE -- this deal is reportedly worth \\$5 billion. Like any large entity, WWE is not without competition, drama, and scandal. \n",
        "\n",
        "## General Tips\n",
        "\n",
        "This is very much a step-by-step process. Don't go crazy trying to get everything done with as few lines as possible. Read the documentation for the AlphaVantage api! Carefully explore the pages from cagematch. There isn't a need to get too fancy with anything here -- just go with simple function and all should be good. Don't print comments, but use normal text for explanations.\n",
        "\n",
        "## Step 1\n",
        "\n",
        "In the `calls` folder, you'll find 4 text files -- these are transcripts from quarterly earnings calls. Read those files in (glob.glob will be very helpful here), with appropriate column names for ticker, quarter, and year columns; this should be done within a single function. Perform any data cleaning that you find necessary. \n"
      ],
      "id": "2730b5c7"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import glob as glob\n",
        "import pandas as pd\n",
        "\n",
        "glob.glob(\"G:/My Drive/MSSA60250 Unstructured Seth/unstructured_notes/calls/*\")\n",
        "\n",
        "def read_calls(folder_path):\n",
        "    files = glob.glob(folder_path + \"/*.txt\")\n",
        "    data = []\n",
        "    for file in files:\n",
        "        file_name = file.split(\"/\")[-1]  \n",
        "        parts = file_name.split(\"_\")\n",
        "        ticker = parts[0]\n",
        "        quarter = parts[1].replace(\"Q\", \"\")\n",
        "        year = parts[2].replace(\".txt\", \"\")\n",
        "        with open(file, \"r\", encoding=\"utf-8\") as f:\n",
        "            content = f.read()\n",
        "        data.append({\"ticker\": ticker, \"quarter\": quarter, \"year\": year, \"content\": content})\n",
        "    df = pd.DataFrame(data)\n",
        "    \n",
        "    return df\n",
        "\n",
        "folder_path = \"G:/My Drive/MSSA60250 Unstructured Seth/unstructured_notes/calls\"\n",
        "calls_df = read_calls(folder_path)\n",
        "\n",
        "calls_df['ticker'] = calls_df['ticker'].str.replace('calls\\\\', '', regex=False)\n",
        "\n",
        "print(calls_df.head())"
      ],
      "id": "17185614",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2\n",
        "\n",
        "Use the AlphaVantage api to get daily stock prices for WWE and related tickers for the last 5 years -- pay attention to your data. You cannot use any AlphaVantage packages (i.e., you can only use requests to grab the data). Tell me about the general trend that you are seeing. I don't care which viz package you use, but plotly is solid and plotnine is good for ggplot2 users.\n"
      ],
      "id": "68caa6b6"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import plotly.io as pio\n",
        "pio.renderers.default = \"browser\"\n",
        "import requests\n",
        "import pandas as pd\n",
        "import plotly.express as px\n",
        "from datetime import datetime\n",
        "\n",
        "def fetch_stock_data(symbol, api_key):\n",
        "    \"\"\"\n",
        "    Fetch daily stock price data using TIME_SERIES_DAILY (free-tier Alpha Vantage).\n",
        "    Args:\n",
        "        symbol (str): Stock ticker symbol.\n",
        "        api_key (str): AlphaVantage API key.\n",
        "    Returns:\n",
        "        pd.DataFrame: DataFrame with daily stock prices.\n",
        "    \"\"\"\n",
        "    url = f\"https://www.alphavantage.co/query\"\n",
        "    params = {\n",
        "        \"function\": \"TIME_SERIES_DAILY\",  \n",
        "        \"symbol\": symbol,\n",
        "        \"apikey\": api_key,\n",
        "        \"outputsize\": \"full\"  \n",
        "    }\n",
        "    \n",
        "    response = requests.get(url, params=params)\n",
        "    data = response.json()\n",
        "    \n",
        "    if \"Time Series (Daily)\" not in data:\n",
        "        print(f\"Error fetching data for {symbol}: {data.get('Error Message', 'Unknown error')}\")\n",
        "        return None\n",
        "    \n",
        "    daily_prices = data[\"Time Series (Daily)\"]\n",
        "    df = pd.DataFrame.from_dict(daily_prices, orient=\"index\")\n",
        "    df = df.reset_index().rename(columns={\"index\": \"date\"})\n",
        "    df[\"date\"] = pd.to_datetime(df[\"date\"])\n",
        "    df = df.sort_values(\"date\")\n",
        "    \n",
        "    df = df.rename(columns={\n",
        "        \"1. open\": \"open\",\n",
        "        \"2. high\": \"high\",\n",
        "        \"3. low\": \"low\",\n",
        "        \"4. close\": \"close\",\n",
        "        \"5. volume\": \"volume\"\n",
        "    })\n",
        "    df = df[[\"date\", \"close\"]]  \n",
        "    df[\"close\"] = df[\"close\"].astype(float)\n",
        "    \n",
        "    return df\n",
        "\n",
        "api_key = \"YA9LEXMLC8NH72G1\"\n",
        "tickers = [\"TKO\", \"EDR\"]  \n",
        "\n",
        "\n",
        "stock_data = {}\n",
        "for ticker in tickers:\n",
        "    print(f\"Fetching data for {ticker}...\")\n",
        "    df = fetch_stock_data(ticker, api_key)\n",
        "    if df is not None:\n",
        "        stock_data[ticker] = df.assign(ticker=ticker)\n",
        "\n",
        "\n",
        "if stock_data:\n",
        "    combined_data = pd.concat(stock_data.values(), ignore_index=True)\n",
        "    \n",
        "    \n",
        "    five_years_ago = datetime.now() - pd.DateOffset(years=5)\n",
        "    combined_data = combined_data[combined_data[\"date\"] >= five_years_ago]\n",
        "    \n",
        "    fig = px.line(\n",
        "        combined_data,\n",
        "        x=\"date\",\n",
        "        y=\"close\",\n",
        "        color=\"ticker\",\n",
        "        title=\"5-Year Stock Price Trends\",\n",
        "        labels={\"close\": \"Closing Price\", \"date\": \"Date\"},\n",
        "    )\n",
        "    fig.update_layout(xaxis_title=\"Date\", yaxis_title=\"Closing Price\", legend_title=\"Ticker\")\n",
        "    fig.show()\n",
        "else:\n",
        "    print(\"No valid data fetched. Please check your API key or tickers.\")\n",
        "\n",
        "#In this plot, we can see that the stock prices for both TKO and EDR have been increasing over the past 5 years, but TKO has taken a significant jump while EDR is barely rising. On the AlphaVantage API website, I was not able to pull any WWE data."
      ],
      "id": "301698b1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3\n",
        "\n",
        "Just like every other nerdy hobby, professional wrestling draws dedicated fans. Wrestling fans often go to cagematch.net to leave reviews for matches, shows, and wrestlers. The following link contains the top 100 matches on cagematch: https://www.cagematch.net/?id=111&view=statistics\n",
        "\n",
        "* What is the correlation between WON ratings and cagematch ratings?\n",
        "\n",
        "** Which wrestler has the most matches in the top 100?\n",
        "\n",
        "*** Which promotion has the most matches in the top 100? \n",
        "\n",
        "**** What is each promotion's average WON rating?\n",
        "\n",
        "***** Select any single match and get the comments and ratings for that match into a data frame.\n"
      ],
      "id": "1d395d56"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "\n",
        "# URL of the page containing the table\n",
        "url = \"https://www.cagematch.net/?id=111&view=statistics\"\n",
        "\n",
        "# Send a GET request to the webpage\n",
        "response = requests.get(url)\n",
        "\n",
        "# Check if the request was successful\n",
        "if response.status_code == 200:\n",
        "    # Parse the HTML content\n",
        "    soup = BeautifulSoup(response.content, 'html.parser')\n",
        "    \n",
        "    # Locate the div with class \"Table\"\n",
        "    table_div = soup.find(\"div\", {\"class\": \"Table\"})\n",
        "    \n",
        "    if table_div:\n",
        "        # Locate all rows within the table\n",
        "        rows = table_div.find_all(\"tr\")[1:]  # Skip the header row\n",
        "        \n",
        "        # Extract data from each row\n",
        "        data = []\n",
        "        for row in rows:\n",
        "            columns = row.find_all(\"td\")\n",
        "            data.append({\n",
        "                \"Rank\": columns[0].text.strip(),\n",
        "                \"Date\": columns[1].text.strip(),\n",
        "                \"Promotion\": columns[2].img[\"alt\"].strip() if columns[2].img else None,\n",
        "                \"Match\": columns[3].text.strip(),\n",
        "                \"WON Rating\": columns[4].text.strip(),\n",
        "                \"Match Type\": columns[5].text.strip(),\n",
        "                \"Rating\": columns[6].text.strip(),\n",
        "                \"Votes\": columns[7].text.strip()\n",
        "            })\n",
        "        \n",
        "        # Convert to a DataFrame\n",
        "        df = pd.DataFrame(data)\n",
        "        \n",
        "        # Display or save the DataFrame\n",
        "        print(df)"
      ],
      "id": "7749c3e8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Question 1"
      ],
      "id": "133de29d"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#* What is the correlation between WON ratings and cagematch ratings?\n",
        "import re\n",
        "\n",
        "def convert_won_rating(rating):\n",
        "    if not isinstance(rating, str):  \n",
        "        return None\n",
        "    match = re.match(r\"\\*{1,5}(?:\\*\\/\\d|\\*\\d\\/\\d)?\", rating)  \n",
        "    if match:\n",
        "        base = rating.count(\"*\")  \n",
        "        fraction = 0.25 if \"1/4\" in rating else 0.5 if \"1/2\" in rating else 0.75 if \"3/4\" in rating else 0\n",
        "        return base + fraction\n",
        "    return None  \n",
        "\n",
        "df[\"WON Rating Numeric\"] = df[\"WON Rating\"].apply(convert_won_rating)\n",
        "\n",
        "df_cleaned = df.dropna(subset=[\"WON Rating Numeric\", \"Rating\"])\n",
        "\n",
        "correlation = df_cleaned[\"WON Rating Numeric\"].corr(df_cleaned[\"Rating\"])\n",
        "print(f\"Correlation between WON Ratings and Cagematch Ratings: {correlation}\")"
      ],
      "id": "0885cc72",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Question 2"
      ],
      "id": "450b51d5"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#** Which wrestler has the most matches in the top 100?\n",
        "from collections import Counter\n",
        "\n",
        "all_wrestlers = []\n",
        "\n",
        "# Process each match in the dataset\n",
        "for match in df[\"Match\"]:\n",
        "    # Split the match into sides using \" vs. \"\n",
        "    sides = match.split(\" vs. \") if \" vs. \" in match else [match]\n",
        "    \n",
        "    # For each side, split further by \" & \" to account for tag teams\n",
        "    for side in sides:\n",
        "        wrestlers = side.split(\" & \")\n",
        "        all_wrestlers.extend(wrestlers)\n",
        "\n",
        "# Count the occurrences of each wrestler\n",
        "wrestler_counts = Counter(all_wrestlers)\n",
        "\n",
        "# Find the wrestler with the most matches\n",
        "most_common_wrestler = wrestler_counts.most_common(1)[0]\n",
        "print(f\"Wrestler with the most matches: {most_common_wrestler[0]} ({most_common_wrestler[1]} matches)\")"
      ],
      "id": "4018f4a9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Question 3"
      ],
      "id": "4ae22620"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Count occurrences of each promotion\n",
        "promotion_counts = df[\"Promotion\"].value_counts()\n",
        "\n",
        "# Top promotion\n",
        "top_promotion = promotion_counts.idxmax()\n",
        "top_promotion_count = promotion_counts.max()\n",
        "\n",
        "print(f\"Top promotion: {top_promotion} with {top_promotion_count} matches\")"
      ],
      "id": "52a796e2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Question 4"
      ],
      "id": "a759d09b"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#**** What is each promotion's average WON rating?\n",
        "import re\n",
        "\n",
        "def convert_won_rating(rating):\n",
        "    if not isinstance(rating, str):  \n",
        "        return None\n",
        "    match = re.match(r\"\\*{1,5}(?:\\*\\/\\d|\\*\\d\\/\\d)?\", rating)  \n",
        "    if match:\n",
        "        base = rating.count(\"*\")  \n",
        "        fraction = 0.25 if \"1/4\" in rating else 0.5 if \"1/2\" in rating else 0.75 if \"3/4\" in rating else 0\n",
        "        return base + fraction\n",
        "    return None  \n",
        "\n",
        "df[\"WON Rating\"] = df[\"WON Rating\"].apply(convert_won_rating)\n",
        "\n",
        "print(df[\"WON Rating\"].isna().sum(), \"NaN values remain in WON Rating.\")\n",
        "\n",
        "avg_won_ratings = df.groupby(\"Promotion\")[\"WON Rating\"].mean()\n",
        "\n",
        "print(\"Average WON Ratings by Promotion:\")\n",
        "print(avg_won_ratings)"
      ],
      "id": "c8fe1d16",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Question 5"
      ],
      "id": "599369e3"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "##Select any single match and get the comments and ratings for that match into a data frame.\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "\n",
        "# Step 1: Send a GET request to the webpage\n",
        "url = \"https://www.cagematch.net/?id=111&nr=8034&page=99\"\n",
        "response = requests.get(url)\n",
        "\n",
        "# Step 2: Parse the page with BeautifulSoup\n",
        "soup = BeautifulSoup(response.content, \"html.parser\")\n",
        "\n",
        "# Step 3: Find the comment and rating section\n",
        "comments_section = soup.find_all(\"div\", class_=\"Comment\")\n",
        "\n",
        "# Step 4: Initialize a list to store comment data\n",
        "comments_data = []\n",
        "\n",
        "# Step 5: Extract the commenter name, comment content, and any rating\n",
        "for comment in comments_section:\n",
        "    commenter = comment.find(\"a\")  # The name of the commenter is in an <a> tag\n",
        "    comment_text = comment.find(\"div\", class_=\"CommentContents\")  # The actual comment\n",
        "    if commenter and comment_text:\n",
        "        commenter_name = commenter.get_text(strip=True)\n",
        "        comment_content = comment_text.get_text(strip=True)\n",
        "        \n",
        "        # Optional: Check if the comment has a rating number\n",
        "        rating = comment.find(\"span\", class_=\"Rating\")  # Adjust this based on page structure\n",
        "        if rating:\n",
        "            rating_value = rating.get_text(strip=True)\n",
        "        else:\n",
        "            rating_value = \"No rating\"\n",
        "\n",
        "        # Append the data as a dictionary to the list\n",
        "        comments_data.append({\n",
        "            \"Commenter\": commenter_name,\n",
        "            \"Rating\": rating_value,\n",
        "            \"Comment\": comment_content\n",
        "        })\n",
        "\n",
        "# Step 6: Convert the list of dictionaries into a pandas DataFrame\n",
        "df_comments = pd.DataFrame(comments_data)\n",
        "\n",
        "# Step 7: Display the DataFrame\n",
        "print(df_comments)"
      ],
      "id": "aecfc4cb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4\n",
        "\n",
        "You can't have matches without wrestlers. The following link contains the top 100 wrestlers, according to cagematch: https://www.cagematch.net/?id=2&view=statistics\n",
        "\n",
        "*** Of the top 100, who has wrestled the most matches?\n",
        "\n",
        "***** Of the top 100, which wrestler has the best win/loss?\n"
      ],
      "id": "041ad103"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "\n",
        "def scrape_wrestler_links():\n",
        "    \"\"\"\n",
        "    Scrapes the top 100 wrestlers' names and profile links from the main page.\n",
        "    \n",
        "    Returns:\n",
        "        pd.DataFrame: A DataFrame containing wrestler names and profile links.\n",
        "    \"\"\"\n",
        "    url = \"https://www.cagematch.net/?id=2&view=statistics\"\n",
        "    headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
        "    response = requests.get(url, headers=headers)\n",
        "    \n",
        "    if response.status_code != 200:\n",
        "        print(\"Failed to fetch the main page.\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    # Parse the page\n",
        "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
        "    table = soup.find(\"table\", class_=\"TBase TableBorderColor\")  # Updated class name\n",
        "    if not table:\n",
        "        print(\"Table not found!\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    rows = table.find_all(\"tr\", class_=[\"TRow1\", \"TRow2\"])  # Rows with data\n",
        "\n",
        "    data = []\n",
        "    for row in rows:\n",
        "        cols = row.find_all(\"td\")\n",
        "        if len(cols) > 1:  # Ensure the row contains data\n",
        "            name_tag = cols[1].find(\"a\")  # Link to wrestler's profile\n",
        "            if name_tag:\n",
        "                name = name_tag.text.strip()\n",
        "                link = \"https://www.cagematch.net/\" + name_tag[\"href\"]\n",
        "                data.append({\"Wrestler\": name, \"Profile Link\": link})\n",
        "\n",
        "    return pd.DataFrame(data)\n",
        "\n",
        "# Scrape wrestler links\n",
        "wrestler_links_df = scrape_wrestler_links()\n",
        "print(wrestler_links_df.head())"
      ],
      "id": "e13e6c6b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#*** Of the top 100, who has wrestled the most matches?\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from urllib.parse import urlparse, parse_qs, urlencode\n",
        "\n",
        "# Base URL for the website\n",
        "base_url = \"https://www.cagematch.net/\"\n",
        "\n",
        "# URL to scrape the list of wrestlers\n",
        "url = \"https://www.cagematch.net/?id=2&view=statistics\"\n",
        "\n",
        "# Dictionary to store wrestler names and their total matches\n",
        "wrestler_matches = []\n",
        "\n",
        "# Step 1: Request the page with the list\n",
        "response = requests.get(url)\n",
        "soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "# Step 2: Extract all wrestler URLs (hrefs) from the 'Gimmick' column\n",
        "gimmick_links = soup.find_all('a', href=True)\n",
        "wrestler_urls = [base_url + link['href'].replace(\"&amp;\", \"&\") for link in gimmick_links if 'gimmick=' in link['href']]\n",
        "\n",
        "# Step 3: For each wrestler URL, modify the URL and scrape the total matches\n",
        "for wrestler_url in wrestler_urls:\n",
        "    # Parse the URL and remove the gimmick and any other query parameters\n",
        "    parsed_url = urlparse(wrestler_url)\n",
        "    query_params = parse_qs(parsed_url.query)\n",
        "\n",
        "    # Remove the gimmick parameter\n",
        "    query_params.pop('gimmick', None)\n",
        "\n",
        "    # Rebuild the URL without the gimmick parameter and append &page=22\n",
        "    new_query = urlencode(query_params, doseq=True)\n",
        "    wrestler_page_url = f\"{parsed_url.scheme}://{parsed_url.netloc}{parsed_url.path}?{new_query}&page=22\"\n",
        "\n",
        "    # Send the request to the new page URL\n",
        "    wrestler_response = requests.get(wrestler_page_url)\n",
        "    wrestler_soup = BeautifulSoup(wrestler_response.text, 'html.parser')\n",
        "    \n",
        "    # Attempt to find the Total Matches in the div with class 'InformationBoxContents'\n",
        "    total_matches_section = wrestler_soup.find('div', class_='InformationBoxContents')\n",
        "    \n",
        "    if total_matches_section:\n",
        "        # Extract the number (strip whitespace and clean it up)\n",
        "        total_matches = int(total_matches_section.text.strip().split()[0])  # Convert to integer for sorting\n",
        "        wrestler_name = wrestler_url.split('=')[-1]  # Extract wrestler name from the URL (after 'gimmick=')\n",
        "        wrestler_matches.append((wrestler_name, total_matches))\n",
        "    else:\n",
        "        print(f\"Wrestler: {wrestler_url} - Total Matches not found\")\n",
        "\n",
        "# Step 4: Sort the wrestlers by total matches in descending order\n",
        "sorted_wrestlers = sorted(wrestler_matches, key=lambda x: x[1], reverse=True)\n",
        "\n",
        "# Step 5: Print the sorted wrestlers\n",
        "print(\"Wrestlers sorted by Total Matches:\")\n",
        "for wrestler, matches in sorted_wrestlers:\n",
        "    print(f\"{wrestler}: {matches} matches\")\n",
        "\n",
        "\n",
        "### Answer is Ric Flair"
      ],
      "id": "39f32be1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "##***** Of the top 100, which wrestler has the best win/loss?\n",
        "\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from urllib.parse import urlparse, parse_qs, urlencode\n",
        "\n",
        "# Base URL for the website\n",
        "base_url = \"https://www.cagematch.net/\"\n",
        "\n",
        "# URL to scrape the list of wrestlers\n",
        "url = \"https://www.cagematch.net/?id=2&view=statistics\"\n",
        "\n",
        "# List to store wrestler names and their win percentage\n",
        "wrestler_win_percentages = []\n",
        "\n",
        "# Step 1: Request the page with the list\n",
        "response = requests.get(url)\n",
        "soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "# Print the page HTML to check if the structure is correct\n",
        "print(soup.prettify())  # Prints the HTML of the page in a readable format\n",
        "\n",
        "# Step 2: Extract all wrestler URLs (hrefs) from the 'Gimmick' column\n",
        "gimmick_links = soup.find_all('a', href=True)\n",
        "wrestler_urls = [base_url + link['href'].replace(\"&amp;\", \"&\") for link in gimmick_links if 'gimmick=' in link['href']]\n",
        "\n",
        "# Debugging: Print all extracted wrestler URLs to ensure we are collecting them correctly\n",
        "print(\"Wrestler URLs extracted:\")\n",
        "for url in wrestler_urls:\n",
        "    print(url)\n",
        "\n",
        "# Step 3: For each wrestler URL, modify the URL and scrape the win percentage\n",
        "for wrestler_url in wrestler_urls:\n",
        "    # Parse the URL and remove the gimmick and any other query parameters\n",
        "    parsed_url = urlparse(wrestler_url)\n",
        "    query_params = parse_qs(parsed_url.query)\n",
        "\n",
        "    # Remove the gimmick parameter\n",
        "    query_params.pop('gimmick', None)\n",
        "\n",
        "    # Rebuild the URL without the gimmick parameter and append &page=22\n",
        "    new_query = urlencode(query_params, doseq=True)\n",
        "    wrestler_page_url = f\"{parsed_url.scheme}://{parsed_url.netloc}{parsed_url.path}?{new_query}&page=22\"\n",
        "\n",
        "    # Send the request to the new page URL\n",
        "    wrestler_response = requests.get(wrestler_page_url)\n",
        "    wrestler_soup = BeautifulSoup(wrestler_response.text, 'html.parser')\n",
        "\n",
        "    # Debugging: Print the wrestler page HTML to see the structure\n",
        "    # print(f\"HTML for {wrestler_url}:\")\n",
        "    # print(wrestler_soup.prettify())  # Uncomment this if needed for debugging\n",
        "\n",
        "    # Attempt to find the win percentage in the div with class 'InformationBoxContents'\n",
        "    win_percentage_section = wrestler_soup.find_all('div', class_='InformationBoxContents')\n",
        "\n",
        "    # Debugging: Print all the found InformationBoxContents divs to see their content\n",
        "    # print(f\"Found InformationBoxContents divs for {wrestler_url}:\")\n",
        "    # for section in win_percentage_section:\n",
        "    #     print(section.text.strip())\n",
        "\n",
        "    # Find the div that contains the win percentage (format should be \"1195 (49.7%)\")\n",
        "    for section in win_percentage_section:\n",
        "        win_percentage_text = section.text.strip()\n",
        "\n",
        "        # We are looking for the format that includes the percentage in parentheses\n",
        "        if '(' in win_percentage_text and ')' in win_percentage_text:\n",
        "            # Extract the percentage inside parentheses (e.g., \"49.7%\")\n",
        "            start = win_percentage_text.find('(') + 1\n",
        "            end = win_percentage_text.find(')')\n",
        "            win_percentage_str = win_percentage_text[start:end].strip('%')  # Extract percentage without '%'\n",
        "\n",
        "            # Debugging: Print the extracted win percentage\n",
        "            print(f\"Extracted win percentage for {wrestler_url}: {win_percentage_str}%\")\n",
        "\n",
        "            try:\n",
        "                # Convert the win percentage to float and store it\n",
        "                win_percentage = float(win_percentage_str)  # Convert to float\n",
        "                wrestler_name = wrestler_url.split('=')[-1]  # Extract wrestler name from the URL\n",
        "                wrestler_win_percentages.append((wrestler_name, win_percentage))\n",
        "                break  # If we found a valid win percentage, break the loop\n",
        "            except ValueError:\n",
        "                print(f\"Error converting win percentage for {wrestler_url}. Got: {win_percentage_str}\")\n",
        "        else:\n",
        "            print(f\"Win percentage format not found in div for wrestler: {wrestler_url}\")\n",
        "\n",
        "# Step 4: Sort the wrestlers by win percentage in descending order\n",
        "sorted_wrestlers = sorted(wrestler_win_percentages, key=lambda x: x[1], reverse=True)\n",
        "\n",
        "# Step 5: Print the sorted wrestlers\n",
        "print(\"Wrestlers sorted by Win Percentage:\")\n",
        "for wrestler, win_percent in sorted_wrestlers:\n",
        "    print(f\"{wrestler}: {win_percent}% win rate\")\n",
        "\n",
        "### Answer is Gene Okerlund"
      ],
      "id": "e5f1df05",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5\n",
        "\n",
        "With all of this work out of the way, we can start getting down to strategy.\n",
        "\n",
        "First, what talent should WWE pursue? Advise carefully.\n",
        "\n",
        "WWE should prioritize pursuing talent with high win/loss ratios, as these wrestlers demonstrate dominance and appeal to audiences. Wrestlers with high match counts and strong performance metrics bring experience and visibility, making them valuable additions. WWE could target stars like Kazuchika Okada or Will Ospreay from NJPW or AEW, who have a proven track record of success and cross-promotional appeal. Additionally, WWE should consider younger, underrated wrestlers with strong in-ring skills who can grow with the company and enhance its long-term roster.\n",
        "\n",
        "Second, reconcile what you found in steps 3 and 4 with Netflix's relationship with WWE. Use the data from the following page to help make your case: https://wrestlenomics.com/tv-ratings/\n",
        "\n",
        "Netflix’s partnership with WWE positions the platform to attract younger and diverse audiences, aligning with WWE’s digital expansion goals. For this relationship to thrive, WWE must ensure its content sustains strong engagement and viewership metrics. According to Wrestlenomics’ TV ratings data, WWE programs like Raw and SmackDown already dominate key demographics, such as the 18-49 age group. WWE should leverage this momentum by featuring its top-performing talent and creating compelling storylines to maintain viewership levels on Netflix.\n",
        "\n",
        "Third, do you have any further recommendations for WWE?\n",
        "\n",
        "To solidify its growth, WWE should continue expanding globally by collaborating with international wrestling promotions and recruiting talent from emerging markets. Strengthening its women’s division and tag team categories could diversify its offerings and draw new viewers. WWE should also focus on enhancing its storytelling and character development to deepen fan engagement. Finally, leveraging the Netflix platform, WWE can explore exclusive series or documentaries that spotlight its history, top stars, and behind-the-scenes action to attract broader audiences."
      ],
      "id": "c623b2d7"
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)",
      "path": "C:\\Users\\pauly\\AppData\\Roaming\\Python\\share\\jupyter\\kernels\\python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}